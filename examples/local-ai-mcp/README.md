# Local AI MCP Server

è¿™æ˜¯ä¸€ä¸ªæœ¬åœ° AI çš„ MCP (Model Context Protocol) æœåŠ¡å™¨å®ç°ï¼Œä¸“é—¨è®¾è®¡ç”¨äºä¸ mcp-manager å·¥å…·é›†æˆã€‚

## åŠŸèƒ½ç‰¹æ€§

### ğŸ¤– AI å·¥å…·
- **chat_completion** - ä¸æœ¬åœ° AI æ¨¡å‹è¿›è¡Œå¯¹è¯
- **list_models** - åˆ—å‡ºå¯ç”¨çš„ AI æ¨¡å‹
- **check_health** - æ£€æŸ¥ AI æœåŠ¡å™¨å¥åº·çŠ¶æ€
- **generate_embedding** - ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡

### ğŸ“š èµ„æºç®¡ç†
- **ai://models** - è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
- **ai://config** - æŸ¥çœ‹å½“å‰é…ç½®ä¿¡æ¯

### ğŸ’­ æç¤ºæ¨¡æ¿
- **code_review** - ä»£ç å®¡æŸ¥æç¤ºæ¨¡æ¿
- **explain_concept** - æ¦‚å¿µè§£é‡Šæç¤ºæ¨¡æ¿

## æ”¯æŒçš„æœ¬åœ° AI æœåŠ¡

### 1. Ollama
```bash
# å¯åŠ¨ Ollama
ollama serve

# æ‹‰å–æ¨¡å‹
ollama pull llama2
ollama pull codellama
```

### 2. LocalAI
```bash
# ä½¿ç”¨ Docker å¯åŠ¨ LocalAI
docker run -p 8080:8080 --name local-ai -ti localai/localai:latest
```

### 3. text-generation-webui
```bash
# å¯åŠ¨ text-generation-webui
python server.py --api --listen
```

### 4. LM Studio
- ä¸‹è½½å¹¶å¯åŠ¨ LM Studio
- åœ¨è®¾ç½®ä¸­å¯ç”¨ API æœåŠ¡å™¨
- é»˜è®¤ç«¯å£é€šå¸¸æ˜¯ 1234

## å®‰è£…ä¸é…ç½®

### 1. å®‰è£…ä¾èµ–
```bash
cd examples/local-ai-mcp
npm install
```

### 2. é…ç½®ç¯å¢ƒå˜é‡
```bash
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œé…ç½®æ‚¨çš„æœ¬åœ° AI æœåŠ¡å™¨ä¿¡æ¯
```

### 3. å¯åŠ¨æœåŠ¡å™¨
```bash
npm start
```

## åœ¨ mcp-manager ä¸­é…ç½®

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨æ¨¡æ¿
1. æ‰“å¼€ mcp-manager ç•Œé¢
2. ç‚¹å‡» "Add Server" æ ‡ç­¾
3. é€‰æ‹© "Use Template" æ¨¡å¼
4. é€‰æ‹© "Node.js Local Server" æ¨¡æ¿
5. å¡«å†™é…ç½®ä¿¡æ¯

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨é…ç½®
åœ¨ mcp-manager ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®ï¼š

```json
{
  "command": "node",
  "args": ["D:\\MCP\\mcp-manager\\examples\\local-ai-mcp\\index.js"],
  "env": {
    "LOCAL_AI_BASE_URL": "http://localhost:8080",
    "LOCAL_AI_MODEL": "gpt-3.5-turbo",
    "LOCAL_AI_API_KEY": "your-api-key-here"
  }
}
```

### æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ UVX (æ¨è)
å¦‚æœæ‚¨å°†æ­¤é¡¹ç›®å‘å¸ƒä¸º npm åŒ…ï¼š

```json
{
  "command": "uvx",
  "args": ["local-ai-mcp-server"],
  "env": {
    "LOCAL_AI_BASE_URL": "http://localhost:8080",
    "LOCAL_AI_MODEL": "gpt-3.5-turbo"
  }
}
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬å¯¹è¯
```javascript
// è°ƒç”¨ chat_completion å·¥å…·
{
  "name": "local-ai_chat_completion",
  "arguments": {
    "message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ MCP åè®®",
    "temperature": 0.7,
    "max_tokens": 500
  }
}
```

### ä»£ç å®¡æŸ¥
```javascript
// ä½¿ç”¨ä»£ç å®¡æŸ¥æç¤ºæ¨¡æ¿
{
  "name": "local-ai_code_review",
  "arguments": {
    "code": "function add(a, b) { return a + b; }",
    "language": "javascript"
  }
}
```

### å¥åº·æ£€æŸ¥
```javascript
// æ£€æŸ¥ AI æœåŠ¡å™¨çŠ¶æ€
{
  "name": "local-ai_check_health",
  "arguments": {}
}
```

## å¸¸è§é…ç½®

### Ollama é…ç½®
```env
LOCAL_AI_BASE_URL=http://localhost:11434
LOCAL_AI_MODEL=llama2
LOCAL_AI_API_KEY=
```

### LocalAI é…ç½®
```env
LOCAL_AI_BASE_URL=http://localhost:8080
LOCAL_AI_MODEL=ggml-gpt4all-j
LOCAL_AI_API_KEY=
```

### LM Studio é…ç½®
```env
LOCAL_AI_BASE_URL=http://localhost:1234
LOCAL_AI_MODEL=llama-2-7b-chat
LOCAL_AI_API_KEY=
```

## æ•…éšœæ’é™¤

### 1. è¿æ¥å¤±è´¥
- ç¡®è®¤æœ¬åœ° AI æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ
- æ£€æŸ¥ç«¯å£å’Œåœ°å€é…ç½®
- éªŒè¯é˜²ç«å¢™è®¾ç½®

### 2. æ¨¡å‹ä¸å­˜åœ¨
- ä½¿ç”¨ `list_models` å·¥å…·æŸ¥çœ‹å¯ç”¨æ¨¡å‹
- ç¡®è®¤æ¨¡å‹åç§°æ‹¼å†™æ­£ç¡®
- æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½

### 3. API å¯†é’¥é”™è¯¯
- æŸäº›æœ¬åœ°æœåŠ¡ä¸éœ€è¦ API å¯†é’¥ï¼Œå¯ç•™ç©º
- æ£€æŸ¥ API å¯†é’¥æ ¼å¼å’Œæƒé™

## æ‰©å±•å¼€å‘

æ‚¨å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•æ›´å¤šå·¥å…·ï¼š

```javascript
// æ·»åŠ æ–°å·¥å…·
{
  name: 'summarize_text',
  description: 'æ–‡æœ¬æ‘˜è¦ç”Ÿæˆ',
  inputSchema: {
    type: 'object',
    properties: {
      text: {
        type: 'string',
        description: 'è¦æ‘˜è¦çš„æ–‡æœ¬'
      },
      length: {
        type: 'string',
        enum: ['short', 'medium', 'long'],
        description: 'æ‘˜è¦é•¿åº¦'
      }
    },
    required: ['text']
  }
}
```

## è®¸å¯è¯

MIT License